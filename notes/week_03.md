# Week 03 – Deep Learning Foundations (Neural Networks)

## Dates: June 24 – June 30, 2025  
## Phase: Month 2 → Month 3 Transition  
**Platform:** [DeepLearning.AI TensorFlow Intro](https://www.coursera.org/learn/introduction-tensorflow) or [PyTorch Basics](https://pytorch.org/tutorials/)

---

## Weekly Goals
- Understand the architecture of a neural network
- Learn about activation functions, weights, biases, and loss
- Train a basic model to recognize handwritten digits (MNIST)
- Document and reflect on neural network performance and tuning

---

## Daily Breakdown

| Day | Topic                                       | Status | Notes/Reflection |
|-----|---------------------------------------------|--------|------------------|
| 1   | What is a Neural Network?                   |        |                  |
| 2   | Forward Propagation + Activation Functions  |        |                  |
| 3   | Backpropagation + Loss Functions            |        |                  |
| 4   | Build First NN with TensorFlow or PyTorch   |        |                  |
| 5   | Train on MNIST + Adjust Hyperparameters     |        |                  |
| 6   | Evaluate & Visualize Predictions            |        |                  |
| 7   | Write Reflections + Week Review             |        |                  |

---

## Key Learnings

- Role of hidden layers and why deep networks matter
- How gradient descent optimizes weights
- Impact of batch size, epochs, and learning rate
- Use of `ReLU`, `Sigmoid`, `Softmax` activations

---

## Project Tracker: MNIST Digit Recognizer

- [ ] Choose framework: TensorFlow or PyTorch
- [ ] Load `MNIST` dataset and normalize data
- [ ] Build and compile a simple dense neural net
- [ ] Train with `fit()` and monitor accuracy/loss
- [ ] Plot training curves using `matplotlib`
- [ ] Save model and push project to GitHub

---

> “You don’t fully understand a neural network until you’ve watched it overfit on MNIST.”
